# 神经网络架构详解

## 章节概述

神经网络架构决定了模型如何**处理不同类型的数据和任务**。本章将深入解析全连接网络、卷积网络、循环网络等主流架构的设计原理、适用场景和实现细节，帮助理解不同架构的**内在逻辑和优势**。

## 前向传播与反向传播


## 核心知识点分点详解

### 1. 全连接神经网络（Fully Connected Network）

**基本结构特点**
- **每个神经元与下一层所有神经元相连**
- **参数数量庞大**：$参数数 = 输入维度 × 输出维度$
- **适合处理表格数据**，不适合图像、序列等结构化数据

**数学表示**
$$
\mathbf{h} = f(\mathbf{W} \mathbf{x} + \mathbf{b})
$$
- $\mathbf{x}$：输入向量
- $\mathbf{W}$：权重矩阵
- $\mathbf{b}$：偏置向量
- $f$：激活函数



### 2. 卷积神经网络（Convolutional Neural Network）

**卷积操作原理**
- **局部连接**：每个神经元只连接输入的一小片区域
- **权重共享**：同一个卷积核在整个输入上滑动使用
- **平移不变性**：特征检测与位置无关

**卷积计算公式**
$$
(\mathbf{I} * \mathbf{K})_{i,j} = \sum_{m} \sum_{n} \mathbf{I}_{i+m,j+n} \cdot \mathbf{K}_{m,n}
$$
- $\mathbf{I}$：输入特征图
- $\mathbf{K}$：卷积核
- $*$：卷积操作

**池化层作用**
- **降维**：减少参数数量和计算量
- **增强平移不变性**：对微小位置变化不敏感
- **常用方法**：最大池化、平均池化

**CNN典型架构**
```
输入图像 → 卷积层 → 激活函数 → 池化层 → 重复多次 → 全连接层 → 输出
```

### 3. 循环神经网络（Recurrent Neural Network）

**序列数据处理原理**
- **时间维度上的循环连接**
- **记忆机制**：隐藏状态传递历史信息
- **变长序列处理**：适应不同长度的输入

**RNN数学公式**
$$
\mathbf{h}_t = f(\mathbf{W}_h \mathbf{h}_{t-1} + \mathbf{W}_x \mathbf{x}_t + \mathbf{b})
$$
- $\mathbf{h}_t$：时刻$t$的隐藏状态
- $\mathbf{x}_t$：时刻$t$的输入
- $\mathbf{W}_h, \mathbf{W}_x$：权重矩阵

**梯度消失/爆炸问题**
- **原因**：长期依赖导致梯度指数级衰减或增长
- **解决方案**：LSTM、GRU等门控机制

### 4. 长短期记忆网络（LSTM）

**LSTM核心组件**
- **遗忘门**：决定保留多少历史信息
  $$
  f_t = \sigma(\mathbf{W}_f [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f)
  $$
- **输入门**：决定更新多少新信息
  $$
  i_t = \sigma(\mathbf{W}_i [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i)
  $$
- **输出门**：决定输出多少信息
  $$
  o_t = \sigma(\mathbf{W}_o [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o)
  $$

**LSTM优势**
- ✅ **长期记忆**：有效解决梯度消失问题
- ✅ **选择性记忆**：自主决定记住和忘记什么

### 5. 注意力机制（Attention Mechanism）

**注意力原理**
- **动态权重分配**：根据输入重要性分配不同注意力
- **上下文感知**：考虑输入间的相互关系

**注意力计算**
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$
- $Q$：查询向量（Query）
- $K$：键向量（Key）
- $V$：值向量（Value）
- $d_k$：向量维度，用于缩放

### 6. Transformer架构

**自注意力机制**
- **并行计算**：替代RNN的串行处理
- **全局依赖**：直接建模任意位置间的关系

**Transformer组件**
- **编码器**：处理输入序列，提取特征
- **解码器**：生成输出序列
- **位置编码**：注入位置信息，弥补无顺序性










