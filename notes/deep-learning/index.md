# Deep Learning Comprehensive Notes

欢迎来到深度学习综合笔记库。本目录提供深度学习核心概念的深入理论分析、数学推导和实用PyTorch实现。

## 内容概览

### 1. [优化算法 (Optimizers)](01-optimizers.md)
- **数学基础**：梯度下降理论、收敛性分析
- **算法详解**：SGD、Momentum、Adam、RMSProp等
- **二阶方法**：牛顿法、拟牛顿法
- **实践实现**：完整PyTorch代码与性能对比
- **延伸学习**：超参数调优、学习率调度

**核心特点**：
- 详细的梯度推导和收敛性证明
- 自适应学习率方法的数学分析
- 实际训练中的优化技巧

### 2. [激活函数 (Activation Functions)](02-activations.md)
- **生物学启发**：神经元放电机制、神经科学基础
- **数学分析**：Sigmoid、Tanh、ReLU族、Swish、GELU的梯度推导
- **梯度流分析**：死亡神经元问题、梯度消失/爆炸
- **性能对比**：11种激活函数的基准测试
- **选择指南**：根据任务和网络结构的推荐

**核心特点**：
- 生物学机制与深度学习模型的对应关系
- 完整的梯度计算和数值稳定性分析
- 现代激活函数的研究进展

### 3. [卷积神经网络 (CNN)](03-cnn.md)
- **视觉神经科学**：视觉皮层层次结构、感受野机制
- **数学基础**：离散卷积公式、池化操作分析
- **架构演进**：LeNet、AlexNet、VGG、ResNet详细分析
- **现代技术**：深度可分离卷积、空洞卷积、注意力机制
- **完整实现**：自定义CNN架构与特征可视化

**核心特点**：
- 从生物学启发到数学形式化的完整推导
- 主流CNN架构的对比分析和可视化
- 实用训练技巧和数据增强策略

### 4. [循环神经网络家族 (RNN Family)](04-rnn-family.md)
- **生物学基础**：短期记忆神经机制、门控对应关系
- **数学推导**：RNN梯度消失问题、LSTM/GRU门控机制
- **架构对比**：参数数量、门控功能详细比较
- **完整实现**：手动实现RNN、LSTM、GRU
- **应用分析**：长序列依赖测试、训练效率比较

**核心特点**：
- LSTM门控机制与大脑记忆过程的生物学对应
- 梯度流的严格数学分析
- 三种变体的性能全面对比

### 5. [Transformer与自注意力 (Transformer)](05-transformer.md)
- **注意力机制**：生物学启发、数学形式化
- **架构详解**：编码器-解码器结构、位置编码
- **梯度推导**：自注意力机制的完整梯度计算
- **完整实现**：从基础模块到完整Transformer
- **现代应用**：BERT、视觉Transformer、多模态任务

**核心特点**：
- 自注意力机制的深入数学分析
- 多头注意力的并行计算优化
- Transformer在各类任务中的应用指南

## 数学深度要求

每个主题都包含：
- **公式推导**：从基本原理出发的完整数学推导
- **梯度计算**：损失函数对各参数的梯度分析
- **收敛性证明**：优化算法的理论保证
- **数值稳定性**：实际实现中的数值考虑

## 实践代码特色

- **模块化设计**：每个算法都有独立的可复用模块
- **性能基准测试**：不同方法的客观性能对比
- **可视化工具**：特征可视化、梯度分析、收敛曲线
- **调试工具**：死亡神经元检测、梯度分布分析

## 延伸学习资源

每个主题末包含：
- **核心论文**：领域内里程碑式的研究论文
- **开源项目**：相关的优秀开源实现
- **进阶阅读**：当前研究前沿和未来方向
- **应用指南**：实际项目中的选择建议

## 图像资源

- [损失曲线对比](images/loss-curves.svg) - 优化器收敛性能可视化
- [注意力机制示意图](images/attention-mech.png) - Transformer自注意力结构

## 使用建议

1. **理论学习**：按顺序阅读，建立完整的知识体系
2. **代码实践**：运行提供的示例代码，理解实现细节
3. **项目应用**：根据具体任务选择合适的算法和参数
4. **深入研究**：参考延伸学习资源进行专题研究

## 贡献与反馈

本笔记库持续更新，欢迎提出改进建议或贡献新的内容。